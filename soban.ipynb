{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional, BatchNormalization, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./CSV_Files/glucose_data.csv\")\n",
    "if 'Glucose_time' not in data.columns:\n",
    "    data[\"Glucose_time\"] = pd.to_datetime(data['reading_time'], unit='ms')\n",
    "    data['Glucose_time'] = pd.to_datetime(data['Glucose_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "    data = data.drop(columns=['reading_time'])\n",
    "    data = data.sort_values(by='Glucose_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
    "data[\"Glucose_time\"] = pd.to_datetime(data[\"Glucose_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reading</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glucose_time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:10:00</th>\n",
       "      <td>108.1092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:15:00</th>\n",
       "      <td>200.5546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:20:00</th>\n",
       "      <td>293.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:25:00</th>\n",
       "      <td>293.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:30:00</th>\n",
       "      <td>296.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      reading\n",
       "Glucose_time                 \n",
       "2019-08-26 19:10:00  108.1092\n",
       "2019-08-26 19:15:00  200.5546\n",
       "2019-08-26 19:20:00  293.0000\n",
       "2019-08-26 19:25:00  293.0000\n",
       "2019-08-26 19:30:00  296.0000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data[['Glucose_time', 'reading']]\n",
    "df.set_index('Glucose_time', inplace=True)\n",
    "df = df.resample('5min').mean().interpolate(method='linear')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./CSV_Files/glucose_data_resampled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Glucose_time</th>\n",
       "      <th>reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-08-26 19:10:00</td>\n",
       "      <td>108.1092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-08-26 19:15:00</td>\n",
       "      <td>200.5546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-08-26 19:20:00</td>\n",
       "      <td>293.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-08-26 19:25:00</td>\n",
       "      <td>293.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-08-26 19:30:00</td>\n",
       "      <td>296.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Glucose_time   reading\n",
       "0  2019-08-26 19:10:00  108.1092\n",
       "1  2019-08-26 19:15:00  200.5546\n",
       "2  2019-08-26 19:20:00  293.0000\n",
       "3  2019-08-26 19:25:00  293.0000\n",
       "4  2019-08-26 19:30:00  296.0000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./CSV_Files/glucose_data_resampled.csv\")\n",
    "# Drop all the columns which have unnamed in them\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If reading_time is present in the dataset, then convert it to a proper date time format\n",
    "if 'Glucose_time' not in df.columns:\n",
    "    df['reading_time'] = pd.to_datetime(df['reading_time'], unit='ms')\n",
    "    df['reading_time'] = pd.to_datetime(df['reading_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "    df = df.rename(columns={'reading_time': 'Glucose_time'})\n",
    "    df = df.sort_values(by='Glucose_time')\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reading</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glucose_time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:10:00</th>\n",
       "      <td>108.1092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:15:00</th>\n",
       "      <td>200.5546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:20:00</th>\n",
       "      <td>293.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:25:00</th>\n",
       "      <td>293.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:30:00</th>\n",
       "      <td>296.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      reading\n",
       "Glucose_time                 \n",
       "2019-08-26 19:10:00  108.1092\n",
       "2019-08-26 19:15:00  200.5546\n",
       "2019-08-26 19:20:00  293.0000\n",
       "2019-08-26 19:25:00  293.0000\n",
       "2019-08-26 19:30:00  296.0000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the Glucose_time to datetime format and set it as the index\n",
    "df['Glucose_time'] = pd.to_datetime(df['Glucose_time'])\n",
    "df.set_index('Glucose_time', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reading</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glucose_time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:10:00</th>\n",
       "      <td>0.146562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:15:00</th>\n",
       "      <td>0.484365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:20:00</th>\n",
       "      <td>0.822168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:25:00</th>\n",
       "      <td>0.822168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-26 19:30:00</th>\n",
       "      <td>0.833130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      reading\n",
       "Glucose_time                 \n",
       "2019-08-26 19:10:00  0.146562\n",
       "2019-08-26 19:15:00  0.484365\n",
       "2019-08-26 19:20:00  0.822168\n",
       "2019-08-26 19:25:00  0.822168\n",
       "2019-08-26 19:30:00  0.833130"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df['reading'] = scaler.fit_transform(df[['reading']])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1498, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(time_series_data):\n",
    "    X, y = [], []\n",
    "    for i in range(len(time_series_data) - 1):\n",
    "        X.append(time_series_data[i])\n",
    "        y.append(time_series_data[i + 1])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data = df['reading'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feat = 1\n",
    "batch_size = 16\n",
    "patience = 25\n",
    "best_rmse = float('inf')\n",
    "best_parameter = {}\n",
    "best_predictions = None\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    return lr * 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1497,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = prepare_data(time_series_data)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with n_features=1, dropout_rate=0.2, batch_size=16, patience=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMVSI-138\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 83ms/step - loss: 0.1895 - val_loss: 0.0939 - learning_rate: 9.9500e-04\n",
      "Epoch 2/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0376 - val_loss: 0.0637 - learning_rate: 9.9003e-04\n",
      "Epoch 3/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0224 - val_loss: 0.0505 - learning_rate: 9.8507e-04\n",
      "Epoch 4/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0196 - val_loss: 0.0492 - learning_rate: 9.8015e-04\n",
      "Epoch 5/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0156 - val_loss: 0.0523 - learning_rate: 9.7525e-04\n",
      "Epoch 6/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0137 - val_loss: 0.0427 - learning_rate: 9.7037e-04\n",
      "Epoch 7/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0127 - val_loss: 0.0343 - learning_rate: 9.6552e-04\n",
      "Epoch 8/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0120 - val_loss: 0.0234 - learning_rate: 9.6069e-04\n",
      "Epoch 9/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.0108 - val_loss: 0.0127 - learning_rate: 9.5589e-04\n",
      "Epoch 10/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0099 - val_loss: 0.0164 - learning_rate: 9.5111e-04\n",
      "Epoch 11/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0104 - val_loss: 0.0173 - learning_rate: 9.4635e-04\n",
      "Epoch 12/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0086 - val_loss: 0.0116 - learning_rate: 9.4162e-04\n",
      "Epoch 13/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0085 - val_loss: 0.0093 - learning_rate: 9.3691e-04\n",
      "Epoch 14/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0072 - val_loss: 0.0152 - learning_rate: 9.3223e-04\n",
      "Epoch 15/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - loss: 0.0086 - val_loss: 0.0096 - learning_rate: 9.2757e-04\n",
      "Epoch 16/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.0081 - val_loss: 0.0070 - learning_rate: 9.2293e-04\n",
      "Epoch 17/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0068 - val_loss: 0.0314 - learning_rate: 9.1832e-04\n",
      "Epoch 18/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0065 - val_loss: 0.0148 - learning_rate: 9.1372e-04\n",
      "Epoch 19/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0065 - val_loss: 0.0108 - learning_rate: 9.0916e-04\n",
      "Epoch 20/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 0.0063 - val_loss: 0.0197 - learning_rate: 9.0461e-04\n",
      "Epoch 21/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0067 - val_loss: 0.0083 - learning_rate: 9.0009e-04\n",
      "Epoch 22/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0061 - val_loss: 0.0256 - learning_rate: 8.9559e-04\n",
      "Epoch 23/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0068 - val_loss: 0.0127 - learning_rate: 8.9111e-04\n",
      "Epoch 24/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0069 - val_loss: 0.0057 - learning_rate: 8.8665e-04\n",
      "Epoch 25/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0056 - val_loss: 0.0063 - learning_rate: 8.8222e-04\n",
      "Epoch 26/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - loss: 0.0050 - val_loss: 0.0066 - learning_rate: 8.7781e-04\n",
      "Epoch 27/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - loss: 0.0050 - val_loss: 0.0069 - learning_rate: 8.7342e-04\n",
      "Epoch 28/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0061 - val_loss: 0.0058 - learning_rate: 8.6905e-04\n",
      "Epoch 29/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - loss: 0.0046 - val_loss: 0.0062 - learning_rate: 8.6471e-04\n",
      "Epoch 30/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0048 - val_loss: 0.0062 - learning_rate: 8.6038e-04\n",
      "Epoch 31/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0051 - val_loss: 0.0033 - learning_rate: 8.5608e-04\n",
      "Epoch 32/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0040 - val_loss: 0.0033 - learning_rate: 8.5180e-04\n",
      "Epoch 33/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0042 - val_loss: 0.0054 - learning_rate: 8.4754e-04\n",
      "Epoch 34/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0048 - val_loss: 0.0042 - learning_rate: 8.4331e-04\n",
      "Epoch 35/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0046 - val_loss: 0.0036 - learning_rate: 8.3909e-04\n",
      "Epoch 36/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0042 - val_loss: 0.0096 - learning_rate: 8.3489e-04\n",
      "Epoch 37/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0030 - learning_rate: 8.3072e-04\n",
      "Epoch 38/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0044 - val_loss: 0.0027 - learning_rate: 8.2657e-04\n",
      "Epoch 39/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0047 - val_loss: 0.0039 - learning_rate: 8.2243e-04\n",
      "Epoch 40/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0039 - val_loss: 0.0050 - learning_rate: 8.1832e-04\n",
      "Epoch 41/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.0042 - val_loss: 0.0049 - learning_rate: 8.1423e-04\n",
      "Epoch 42/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.0039 - val_loss: 0.0085 - learning_rate: 8.1016e-04\n",
      "Epoch 43/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0037 - val_loss: 0.0122 - learning_rate: 8.0611e-04\n",
      "Epoch 44/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0048 - val_loss: 0.0044 - learning_rate: 8.0208e-04\n",
      "Epoch 45/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0040 - val_loss: 0.0058 - learning_rate: 7.9807e-04\n",
      "Epoch 46/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0047 - val_loss: 0.0040 - learning_rate: 7.9408e-04\n",
      "Epoch 47/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0062 - learning_rate: 7.9011e-04\n",
      "Epoch 48/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0044 - val_loss: 0.0061 - learning_rate: 7.8615e-04\n",
      "Epoch 49/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.0040 - val_loss: 0.0028 - learning_rate: 7.8222e-04\n",
      "Epoch 50/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.0031 - val_loss: 0.0041 - learning_rate: 7.7831e-04\n",
      "Epoch 51/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0057 - learning_rate: 7.7442e-04\n",
      "Epoch 52/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0030 - val_loss: 0.0053 - learning_rate: 7.7055e-04\n",
      "Epoch 53/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0032 - val_loss: 0.0045 - learning_rate: 7.6670e-04\n",
      "Epoch 54/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0039 - val_loss: 0.0022 - learning_rate: 7.6286e-04\n",
      "Epoch 55/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0021 - learning_rate: 7.5905e-04\n",
      "Epoch 56/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0030 - val_loss: 0.0019 - learning_rate: 7.5525e-04\n",
      "Epoch 57/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0017 - learning_rate: 7.5148e-04\n",
      "Epoch 58/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0036 - val_loss: 0.0021 - learning_rate: 7.4772e-04\n",
      "Epoch 59/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0036 - val_loss: 0.0041 - learning_rate: 7.4398e-04\n",
      "Epoch 60/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0035 - val_loss: 0.0039 - learning_rate: 7.4026e-04\n",
      "Epoch 61/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 0.0030 - val_loss: 0.0020 - learning_rate: 7.3656e-04\n",
      "Epoch 62/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0032 - val_loss: 0.0027 - learning_rate: 7.3288e-04\n",
      "Epoch 63/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0018 - learning_rate: 7.2921e-04\n",
      "Epoch 64/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0035 - val_loss: 0.0029 - learning_rate: 7.2557e-04\n",
      "Epoch 65/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0023 - learning_rate: 7.2194e-04\n",
      "Epoch 66/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0018 - learning_rate: 7.1833e-04\n",
      "Epoch 67/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0031 - val_loss: 0.0037 - learning_rate: 7.1474e-04\n",
      "Epoch 68/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0033 - val_loss: 0.0034 - learning_rate: 7.1116e-04\n",
      "Epoch 69/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.0029 - learning_rate: 7.0761e-04\n",
      "Epoch 70/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0043 - learning_rate: 7.0407e-04\n",
      "Epoch 71/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0034 - val_loss: 0.0016 - learning_rate: 7.0055e-04\n",
      "Epoch 72/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.0027 - val_loss: 0.0029 - learning_rate: 6.9705e-04\n",
      "Epoch 73/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0028 - val_loss: 0.0025 - learning_rate: 6.9356e-04\n",
      "Epoch 74/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0027 - val_loss: 0.0023 - learning_rate: 6.9009e-04\n",
      "Epoch 75/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0011 - learning_rate: 6.8664e-04\n",
      "Epoch 76/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0028 - val_loss: 0.0220 - learning_rate: 6.8321e-04\n",
      "Epoch 77/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0035 - learning_rate: 6.7979e-04\n",
      "Epoch 78/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0020 - val_loss: 0.0041 - learning_rate: 6.7639e-04\n",
      "Epoch 79/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0027 - learning_rate: 6.7301e-04\n",
      "Epoch 80/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0016 - learning_rate: 6.6965e-04\n",
      "Epoch 81/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0010 - learning_rate: 6.6630e-04\n",
      "Epoch 82/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.0024 - val_loss: 9.2680e-04 - learning_rate: 6.6297e-04\n",
      "Epoch 83/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.0024 - val_loss: 9.4159e-04 - learning_rate: 6.5965e-04\n",
      "Epoch 84/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0023 - val_loss: 0.0145 - learning_rate: 6.5636e-04\n",
      "Epoch 85/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0055 - learning_rate: 6.5307e-04\n",
      "Epoch 86/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0022 - val_loss: 0.0025 - learning_rate: 6.4981e-04\n",
      "Epoch 87/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0025 - val_loss: 0.0020 - learning_rate: 6.4656e-04\n",
      "Epoch 88/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.0026 - learning_rate: 6.4333e-04\n",
      "Epoch 89/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0027 - val_loss: 0.0016 - learning_rate: 6.4011e-04\n",
      "Epoch 90/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0021 - learning_rate: 6.3691e-04\n",
      "Epoch 91/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0029 - learning_rate: 6.3372e-04\n",
      "Epoch 92/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.0013 - learning_rate: 6.3056e-04\n",
      "Epoch 93/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0022 - val_loss: 0.0020 - learning_rate: 6.2740e-04\n",
      "Epoch 94/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0036 - learning_rate: 6.2427e-04\n",
      "Epoch 95/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0027 - val_loss: 0.0013 - learning_rate: 6.2114e-04\n",
      "Epoch 96/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 0.0024 - learning_rate: 6.1804e-04\n",
      "Epoch 97/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0022 - val_loss: 0.0011 - learning_rate: 6.1495e-04\n",
      "Epoch 98/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.0023 - val_loss: 0.0038 - learning_rate: 6.1187e-04\n",
      "Epoch 99/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0028 - learning_rate: 6.0881e-04\n",
      "Epoch 100/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0024 - val_loss: 0.0029 - learning_rate: 6.0577e-04\n",
      "Epoch 101/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0012 - learning_rate: 6.0274e-04\n",
      "Epoch 102/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - loss: 0.0022 - val_loss: 0.0017 - learning_rate: 5.9973e-04\n",
      "Epoch 103/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 0.0024 - val_loss: 0.0019 - learning_rate: 5.9673e-04\n",
      "Epoch 104/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0025 - val_loss: 0.0023 - learning_rate: 5.9375e-04\n",
      "Epoch 105/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - loss: 0.0019 - val_loss: 0.0018 - learning_rate: 5.9078e-04\n",
      "Epoch 106/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0026 - val_loss: 0.0017 - learning_rate: 5.8782e-04\n",
      "Epoch 107/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0026 - val_loss: 9.4094e-04 - learning_rate: 5.8488e-04\n",
      "Epoch 107: early stopping\n",
      "Restoring model weights from the end of the best epoch: 82.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "RMSE: 5856.942012581135\n",
      "Best parameters: {'n_features': 1, 'dropout_rate': 0.2, 'batch_size': 16, 'patience': 25}\n",
      "Best RMSE: 5856.942012581135\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Training model with n_features={n_feat}, dropout_rate=0.2, batch_size={batch_size}, patience={patience}\")\n",
    "train_data_scaled = time_series_data[:-20]\n",
    "test_data_scaled = time_series_data[-(n_feat + 20):]\n",
    "X_train, y_train = prepare_data(train_data_scaled)\n",
    "X_test, y_test = prepare_data(test_data_scaled)\n",
    "X_train = X_train.reshape(X_train.shape[0],1, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 1)\n",
    "# Define the model\n",
    "                \n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=1, activation='relu', input_shape=(1, 1)))\n",
    "model.add(MaxPooling1D(pool_size=1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=200, activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(units=150, activation='relu', return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(units=100, activation='relu', return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(units=50, activation='relu', return_sequences=False))  # Set return_sequences to False\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "        # Compile the model\n",
    "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=25, verbose=1, restore_best_weights=True)\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "        # Train the model\n",
    "history = model.fit(X_train, y_train, epochs=500, batch_size=batch_size, verbose=1, validation_split=0.2, callbacks=[early_stop, lr_scheduler])\n",
    "predictions = []\n",
    "curr_sequence = X_test[0].reshape(1, n_feat, 1)\n",
    "for i in range(20):\n",
    "        next_pred = model.predict(curr_sequence)\n",
    "        predictions.append(next_pred[0, 0])\n",
    "                # Update the sequence: drop the first value and add the new prediction\n",
    "        curr_sequence = np.append(curr_sequence[:, 1:, :], next_pred.reshape(1, 1, 1), axis=1)\n",
    "predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
    "actual = checked_df['reading'].values[-20:]\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(actual, predictions))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_parameter = {'n_features': n_feat, 'dropout_rate': 0.2, 'batch_size': batch_size, 'patience': patience}\n",
    "        best_predictions = predictions\n",
    "        best_model = model\n",
    "\n",
    "print(f\"Best parameters: {best_parameter}\")\n",
    "print(f\"Best RMSE: {best_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>333.444444</td>\n",
       "      <td>9.166655e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>337.555556</td>\n",
       "      <td>9.376454e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>341.666667</td>\n",
       "      <td>9.676735e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>337.666667</td>\n",
       "      <td>1.010626e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>336.000000</td>\n",
       "      <td>1.077100e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>330.000000</td>\n",
       "      <td>1.196503e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>324.000000</td>\n",
       "      <td>1.448281e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>318.000000</td>\n",
       "      <td>2.008922e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>312.500000</td>\n",
       "      <td>3.374102e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>307.000000</td>\n",
       "      <td>7.489483e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>298.333333</td>\n",
       "      <td>1.510460e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>289.666667</td>\n",
       "      <td>1.998508e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>281.000000</td>\n",
       "      <td>2.355257e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>279.000000</td>\n",
       "      <td>2.443280e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>277.000000</td>\n",
       "      <td>2.461732e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>269.000000</td>\n",
       "      <td>2.465492e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>266.750000</td>\n",
       "      <td>2.466254e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>264.500000</td>\n",
       "      <td>2.466409e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>262.250000</td>\n",
       "      <td>2.466440e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>260.000000</td>\n",
       "      <td>2.466446e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Actual     Predicted\n",
       "0   333.444444  9.166655e+04\n",
       "1   337.555556  9.376454e+04\n",
       "2   341.666667  9.676735e+04\n",
       "3   337.666667  1.010626e+05\n",
       "4   336.000000  1.077100e+05\n",
       "5   330.000000  1.196503e+05\n",
       "6   324.000000  1.448281e+05\n",
       "7   318.000000  2.008922e+05\n",
       "8   312.500000  3.374102e+05\n",
       "9   307.000000  7.489483e+05\n",
       "10  298.333333  1.510460e+06\n",
       "11  289.666667  1.998508e+06\n",
       "12  281.000000  2.355257e+06\n",
       "13  279.000000  2.443280e+06\n",
       "14  277.000000  2.461732e+06\n",
       "15  269.000000  2.465492e+06\n",
       "16  266.750000  2.466254e+06\n",
       "17  264.500000  2.466409e+06\n",
       "18  262.250000  2.466440e+06\n",
       "19  260.000000  2.466446e+06"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_predictions = scaler.inverse_transform(np.array(best_predictions).reshape(-1, 1)).flatten()\n",
    "y_test = scaler.inverse_transform(y_test[:20].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Create a DataFrame for actual vs predicted values\n",
    "df_1 = pd.DataFrame({'Actual': y_test, 'Predicted': best_predictions})\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with n_features=1, dropout_rate=0.2, batch_size=16, patience=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMVSI-138\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 39ms/step - loss: 0.1333 - val_loss: 0.0897 - learning_rate: 9.9500e-04\n",
      "Epoch 2/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0343 - val_loss: 0.0624 - learning_rate: 9.9003e-04\n",
      "Epoch 3/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0201 - val_loss: 0.0522 - learning_rate: 9.8507e-04\n",
      "Epoch 4/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0178 - val_loss: 0.0499 - learning_rate: 9.8015e-04\n",
      "Epoch 5/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0169 - val_loss: 0.0496 - learning_rate: 9.7525e-04\n",
      "Epoch 6/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0129 - val_loss: 0.0497 - learning_rate: 9.7037e-04\n",
      "Epoch 7/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0098 - val_loss: 0.0375 - learning_rate: 9.6552e-04\n",
      "Epoch 8/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0095 - val_loss: 0.0256 - learning_rate: 9.6069e-04\n",
      "Epoch 9/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0087 - val_loss: 0.0120 - learning_rate: 9.5589e-04\n",
      "Epoch 10/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0079 - val_loss: 0.0086 - learning_rate: 9.5111e-04\n",
      "Epoch 11/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0073 - val_loss: 0.0089 - learning_rate: 9.4635e-04\n",
      "Epoch 12/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0063 - val_loss: 0.0077 - learning_rate: 9.4162e-04\n",
      "Epoch 13/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.0070 - val_loss: 0.0080 - learning_rate: 9.3691e-04\n",
      "Epoch 14/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0071 - val_loss: 0.0136 - learning_rate: 9.3223e-04\n",
      "Epoch 15/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0060 - val_loss: 0.0204 - learning_rate: 9.2757e-04\n",
      "Epoch 16/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0059 - val_loss: 0.0254 - learning_rate: 9.2293e-04\n",
      "Epoch 17/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0046 - val_loss: 0.0134 - learning_rate: 9.1832e-04\n",
      "Epoch 18/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0059 - val_loss: 0.0214 - learning_rate: 9.1372e-04\n",
      "Epoch 19/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0052 - val_loss: 0.0064 - learning_rate: 9.0916e-04\n",
      "Epoch 20/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 0.0048 - val_loss: 0.0075 - learning_rate: 9.0461e-04\n",
      "Epoch 21/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0049 - val_loss: 0.0122 - learning_rate: 9.0009e-04\n",
      "Epoch 22/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0049 - val_loss: 0.0199 - learning_rate: 8.9559e-04\n",
      "Epoch 23/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.0048 - val_loss: 0.0082 - learning_rate: 8.9111e-04\n",
      "Epoch 24/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0047 - val_loss: 0.0046 - learning_rate: 8.8665e-04\n",
      "Epoch 25/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0044 - val_loss: 0.0301 - learning_rate: 8.8222e-04\n",
      "Epoch 26/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.0036 - val_loss: 0.0039 - learning_rate: 8.7781e-04\n",
      "Epoch 27/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0034 - val_loss: 0.0143 - learning_rate: 8.7342e-04\n",
      "Epoch 28/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0037 - val_loss: 0.0155 - learning_rate: 8.6905e-04\n",
      "Epoch 29/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0041 - val_loss: 0.0117 - learning_rate: 8.6471e-04\n",
      "Epoch 30/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 0.0044 - val_loss: 0.0077 - learning_rate: 8.6038e-04\n",
      "Epoch 31/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0034 - val_loss: 0.0072 - learning_rate: 8.5608e-04\n",
      "Epoch 32/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0038 - val_loss: 0.0130 - learning_rate: 8.5180e-04\n",
      "Epoch 33/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.0044 - val_loss: 0.0057 - learning_rate: 8.4754e-04\n",
      "Epoch 34/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0045 - val_loss: 0.0087 - learning_rate: 8.4331e-04\n",
      "Epoch 35/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0090 - learning_rate: 8.3909e-04\n",
      "Epoch 36/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0037 - val_loss: 0.0030 - learning_rate: 8.3489e-04\n",
      "Epoch 37/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0040 - val_loss: 0.0109 - learning_rate: 8.3072e-04\n",
      "Epoch 38/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0033 - val_loss: 0.0045 - learning_rate: 8.2657e-04\n",
      "Epoch 39/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 0.0032 - val_loss: 0.0155 - learning_rate: 8.2243e-04\n",
      "Epoch 40/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0044 - val_loss: 0.0142 - learning_rate: 8.1832e-04\n",
      "Epoch 41/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0042 - val_loss: 0.0039 - learning_rate: 8.1423e-04\n",
      "Epoch 42/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0049 - val_loss: 0.0033 - learning_rate: 8.1016e-04\n",
      "Epoch 43/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0033 - val_loss: 0.0057 - learning_rate: 8.0611e-04\n",
      "Epoch 44/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0037 - val_loss: 0.0016 - learning_rate: 8.0208e-04\n",
      "Epoch 45/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 0.0036 - val_loss: 0.0050 - learning_rate: 7.9807e-04\n",
      "Epoch 46/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.0035 - val_loss: 0.0025 - learning_rate: 7.9408e-04\n",
      "Epoch 47/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0037 - val_loss: 0.0061 - learning_rate: 7.9011e-04\n",
      "Epoch 48/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0036 - val_loss: 0.0061 - learning_rate: 7.8615e-04\n",
      "Epoch 49/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0033 - val_loss: 0.0072 - learning_rate: 7.8222e-04\n",
      "Epoch 50/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0037 - val_loss: 0.0036 - learning_rate: 7.7831e-04\n",
      "Epoch 51/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0014 - learning_rate: 7.7442e-04\n",
      "Epoch 52/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0040 - val_loss: 0.0105 - learning_rate: 7.7055e-04\n",
      "Epoch 53/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0067 - learning_rate: 7.6670e-04\n",
      "Epoch 54/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0038 - val_loss: 0.0091 - learning_rate: 7.6286e-04\n",
      "Epoch 55/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.0038 - val_loss: 0.0023 - learning_rate: 7.5905e-04\n",
      "Epoch 56/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0032 - val_loss: 0.0037 - learning_rate: 7.5525e-04\n",
      "Epoch 57/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0032 - val_loss: 0.0044 - learning_rate: 7.5148e-04\n",
      "Epoch 58/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.0035 - val_loss: 0.0091 - learning_rate: 7.4772e-04\n",
      "Epoch 59/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.0029 - val_loss: 0.0044 - learning_rate: 7.4398e-04\n",
      "Epoch 60/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0033 - val_loss: 0.0082 - learning_rate: 7.4026e-04\n",
      "Epoch 61/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0048 - learning_rate: 7.3656e-04\n",
      "Epoch 62/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.0030 - val_loss: 0.0032 - learning_rate: 7.3288e-04\n",
      "Epoch 63/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 0.0028 - val_loss: 0.0020 - learning_rate: 7.2921e-04\n",
      "Epoch 64/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0028 - val_loss: 0.0023 - learning_rate: 7.2557e-04\n",
      "Epoch 65/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0029 - val_loss: 0.0051 - learning_rate: 7.2194e-04\n",
      "Epoch 66/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.0031 - val_loss: 0.0027 - learning_rate: 7.1833e-04\n",
      "Epoch 67/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 0.0034 - val_loss: 0.0080 - learning_rate: 7.1474e-04\n",
      "Epoch 68/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0027 - val_loss: 0.0018 - learning_rate: 7.1116e-04\n",
      "Epoch 69/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0028 - val_loss: 0.0030 - learning_rate: 7.0761e-04\n",
      "Epoch 70/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0034 - val_loss: 0.0069 - learning_rate: 7.0407e-04\n",
      "Epoch 71/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0034 - learning_rate: 7.0055e-04\n",
      "Epoch 72/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0029 - val_loss: 0.0231 - learning_rate: 6.9705e-04\n",
      "Epoch 73/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0028 - val_loss: 0.0119 - learning_rate: 6.9356e-04\n",
      "Epoch 74/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0025 - val_loss: 0.0019 - learning_rate: 6.9009e-04\n",
      "Epoch 75/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0033 - learning_rate: 6.8664e-04\n",
      "Epoch 76/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0027 - val_loss: 0.0040 - learning_rate: 6.8321e-04\n",
      "Epoch 77/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0031 - val_loss: 0.0036 - learning_rate: 6.7979e-04\n",
      "Epoch 78/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0035 - val_loss: 0.0094 - learning_rate: 6.7639e-04\n",
      "Epoch 79/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 0.0033 - val_loss: 0.0017 - learning_rate: 6.7301e-04\n",
      "Epoch 80/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0022 - val_loss: 0.0045 - learning_rate: 6.6965e-04\n",
      "Epoch 81/500\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.0029 - val_loss: 0.0165 - learning_rate: 6.6630e-04\n",
      "Epoch 81: early stopping\n",
      "Restoring model weights from the end of the best epoch: 51.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "RMSE: 245.0606801167864\n",
      "Training model with n_features=1, dropout_rate=0.2, batch_size=32, patience=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMVSI-138\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 83ms/step - loss: 0.1530 - val_loss: 0.1032 - learning_rate: 9.9500e-04\n",
      "Epoch 2/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.0313 - val_loss: 0.0756 - learning_rate: 9.9003e-04\n",
      "Epoch 3/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0254 - val_loss: 0.0576 - learning_rate: 9.8507e-04\n",
      "Epoch 4/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0203 - val_loss: 0.0498 - learning_rate: 9.8015e-04\n",
      "Epoch 5/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0170 - val_loss: 0.0496 - learning_rate: 9.7525e-04\n",
      "Epoch 6/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0155 - val_loss: 0.0512 - learning_rate: 9.7037e-04\n",
      "Epoch 7/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0125 - val_loss: 0.0522 - learning_rate: 9.6552e-04\n",
      "Epoch 8/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0102 - val_loss: 0.0543 - learning_rate: 9.6069e-04\n",
      "Epoch 9/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0090 - val_loss: 0.0558 - learning_rate: 9.5589e-04\n",
      "Epoch 10/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0099 - val_loss: 0.0533 - learning_rate: 9.5111e-04\n",
      "Epoch 11/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0079 - val_loss: 0.0505 - learning_rate: 9.4635e-04\n",
      "Epoch 12/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0084 - val_loss: 0.0478 - learning_rate: 9.4162e-04\n",
      "Epoch 13/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0070 - val_loss: 0.0407 - learning_rate: 9.3691e-04\n",
      "Epoch 14/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0091 - val_loss: 0.0333 - learning_rate: 9.3223e-04\n",
      "Epoch 15/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0070 - val_loss: 0.0303 - learning_rate: 9.2757e-04\n",
      "Epoch 16/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.0051 - val_loss: 0.0242 - learning_rate: 9.2293e-04\n",
      "Epoch 17/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0050 - val_loss: 0.0220 - learning_rate: 9.1832e-04\n",
      "Epoch 18/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 0.0057 - val_loss: 0.0164 - learning_rate: 9.1372e-04\n",
      "Epoch 19/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0051 - val_loss: 0.0104 - learning_rate: 9.0916e-04\n",
      "Epoch 20/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.0051 - val_loss: 0.0105 - learning_rate: 9.0461e-04\n",
      "Epoch 21/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.0047 - val_loss: 0.0107 - learning_rate: 9.0009e-04\n",
      "Epoch 22/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.0047 - val_loss: 0.0044 - learning_rate: 8.9559e-04\n",
      "Epoch 23/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.0052 - val_loss: 0.0098 - learning_rate: 8.9111e-04\n",
      "Epoch 24/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.0045 - val_loss: 0.0078 - learning_rate: 8.8665e-04\n",
      "Epoch 25/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 0.0040 - val_loss: 0.0060 - learning_rate: 8.8222e-04\n",
      "Epoch 26/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - loss: 0.0040 - val_loss: 0.0061 - learning_rate: 8.7781e-04\n",
      "Epoch 27/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0050 - val_loss: 0.0151 - learning_rate: 8.7342e-04\n",
      "Epoch 28/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0042 - val_loss: 0.0036 - learning_rate: 8.6905e-04\n",
      "Epoch 29/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.0037 - val_loss: 0.0087 - learning_rate: 8.6471e-04\n",
      "Epoch 30/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0039 - val_loss: 0.0014 - learning_rate: 8.6038e-04\n",
      "Epoch 31/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 0.0043 - val_loss: 0.0052 - learning_rate: 8.5608e-04\n",
      "Epoch 32/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0029 - val_loss: 0.0023 - learning_rate: 8.5180e-04\n",
      "Epoch 33/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0036 - val_loss: 0.0052 - learning_rate: 8.4754e-04\n",
      "Epoch 34/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0033 - val_loss: 0.0023 - learning_rate: 8.4331e-04\n",
      "Epoch 35/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0039 - val_loss: 0.0056 - learning_rate: 8.3909e-04\n",
      "Epoch 36/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0036 - val_loss: 0.0038 - learning_rate: 8.3489e-04\n",
      "Epoch 37/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0032 - val_loss: 0.0013 - learning_rate: 8.3072e-04\n",
      "Epoch 38/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0029 - val_loss: 0.0028 - learning_rate: 8.2657e-04\n",
      "Epoch 39/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0030 - val_loss: 0.0095 - learning_rate: 8.2243e-04\n",
      "Epoch 40/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0032 - val_loss: 0.0075 - learning_rate: 8.1832e-04\n",
      "Epoch 41/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0047 - learning_rate: 8.1423e-04\n",
      "Epoch 42/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0026 - val_loss: 0.0030 - learning_rate: 8.1016e-04\n",
      "Epoch 43/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0029 - val_loss: 0.0055 - learning_rate: 8.0611e-04\n",
      "Epoch 44/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 0.0027 - val_loss: 0.0019 - learning_rate: 8.0208e-04\n",
      "Epoch 45/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.0026 - val_loss: 0.0062 - learning_rate: 7.9807e-04\n",
      "Epoch 46/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0040 - val_loss: 0.0060 - learning_rate: 7.9408e-04\n",
      "Epoch 47/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 0.0026 - val_loss: 0.0031 - learning_rate: 7.9011e-04\n",
      "Epoch 48/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - loss: 0.0029 - val_loss: 0.0023 - learning_rate: 7.8615e-04\n",
      "Epoch 49/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 0.0035 - val_loss: 0.0026 - learning_rate: 7.8222e-04\n",
      "Epoch 50/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0025 - val_loss: 0.0014 - learning_rate: 7.7831e-04\n",
      "Epoch 51/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 0.0028 - val_loss: 0.0012 - learning_rate: 7.7442e-04\n",
      "Epoch 52/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0026 - val_loss: 0.0034 - learning_rate: 7.7055e-04\n",
      "Epoch 53/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0027 - val_loss: 0.0016 - learning_rate: 7.6670e-04\n",
      "Epoch 54/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0021 - val_loss: 0.0035 - learning_rate: 7.6286e-04\n",
      "Epoch 55/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0026 - val_loss: 0.0026 - learning_rate: 7.5905e-04\n",
      "Epoch 56/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0029 - val_loss: 0.0087 - learning_rate: 7.5525e-04\n",
      "Epoch 57/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.0026 - val_loss: 0.0017 - learning_rate: 7.5148e-04\n",
      "Epoch 58/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 0.0024 - val_loss: 0.0025 - learning_rate: 7.4772e-04\n",
      "Epoch 59/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 0.0021 - val_loss: 0.0014 - learning_rate: 7.4398e-04\n",
      "Epoch 60/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 0.0088 - learning_rate: 7.4026e-04\n",
      "Epoch 61/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.0027 - val_loss: 0.0022 - learning_rate: 7.3656e-04\n",
      "Epoch 62/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0022 - val_loss: 0.0012 - learning_rate: 7.3288e-04\n",
      "Epoch 63/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0020 - val_loss: 0.0011 - learning_rate: 7.2921e-04\n",
      "Epoch 64/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0024 - val_loss: 0.0012 - learning_rate: 7.2557e-04\n",
      "Epoch 65/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0023 - val_loss: 0.0016 - learning_rate: 7.2194e-04\n",
      "Epoch 66/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0026 - val_loss: 0.0053 - learning_rate: 7.1833e-04\n",
      "Epoch 67/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0026 - val_loss: 0.0046 - learning_rate: 7.1474e-04\n",
      "Epoch 68/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0025 - val_loss: 0.0033 - learning_rate: 7.1116e-04\n",
      "Epoch 69/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0025 - learning_rate: 7.0761e-04\n",
      "Epoch 70/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.0027 - val_loss: 0.0034 - learning_rate: 7.0407e-04\n",
      "Epoch 71/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.0024 - val_loss: 0.0017 - learning_rate: 7.0055e-04\n",
      "Epoch 72/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0025 - val_loss: 0.0017 - learning_rate: 6.9705e-04\n",
      "Epoch 73/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0068 - learning_rate: 6.9356e-04\n",
      "Epoch 74/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0033 - val_loss: 0.0014 - learning_rate: 6.9009e-04\n",
      "Epoch 75/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0024 - val_loss: 0.0014 - learning_rate: 6.8664e-04\n",
      "Epoch 76/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0054 - learning_rate: 6.8321e-04\n",
      "Epoch 77/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0023 - val_loss: 0.0020 - learning_rate: 6.7979e-04\n",
      "Epoch 78/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0022 - val_loss: 0.0031 - learning_rate: 6.7639e-04\n",
      "Epoch 79/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0024 - val_loss: 0.0038 - learning_rate: 6.7301e-04\n",
      "Epoch 80/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - loss: 0.0024 - val_loss: 0.0011 - learning_rate: 6.6965e-04\n",
      "Epoch 81/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0025 - val_loss: 0.0018 - learning_rate: 6.6630e-04\n",
      "Epoch 82/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 0.0019 - val_loss: 0.0022 - learning_rate: 6.6297e-04\n",
      "Epoch 83/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 0.0024 - val_loss: 0.0016 - learning_rate: 6.5965e-04\n",
      "Epoch 84/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0022 - val_loss: 0.0016 - learning_rate: 6.5636e-04\n",
      "Epoch 85/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0026 - val_loss: 0.0031 - learning_rate: 6.5307e-04\n",
      "Epoch 86/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0018 - learning_rate: 6.4981e-04\n",
      "Epoch 87/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0031 - learning_rate: 6.4656e-04\n",
      "Epoch 88/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - loss: 0.0018 - val_loss: 0.0031 - learning_rate: 6.4333e-04\n",
      "Epoch 89/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0018 - val_loss: 0.0026 - learning_rate: 6.4011e-04\n",
      "Epoch 90/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0018 - val_loss: 0.0022 - learning_rate: 6.3691e-04\n",
      "Epoch 91/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.0020 - learning_rate: 6.3372e-04\n",
      "Epoch 92/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0036 - learning_rate: 6.3056e-04\n",
      "Epoch 93/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0023 - learning_rate: 6.2740e-04\n",
      "Epoch 94/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0025 - val_loss: 0.0024 - learning_rate: 6.2427e-04\n",
      "Epoch 95/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0020 - val_loss: 0.0015 - learning_rate: 6.2114e-04\n",
      "Epoch 96/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 0.0021 - val_loss: 0.0038 - learning_rate: 6.1804e-04\n",
      "Epoch 97/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0021 - val_loss: 0.0024 - learning_rate: 6.1495e-04\n",
      "Epoch 98/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0020 - val_loss: 0.0013 - learning_rate: 6.1187e-04\n",
      "Epoch 99/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0030 - learning_rate: 6.0881e-04\n",
      "Epoch 100/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0021 - val_loss: 9.1326e-04 - learning_rate: 6.0577e-04\n",
      "Epoch 101/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0018 - val_loss: 0.0011 - learning_rate: 6.0274e-04\n",
      "Epoch 102/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - loss: 0.0021 - val_loss: 0.0029 - learning_rate: 5.9973e-04\n",
      "Epoch 103/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.0021 - val_loss: 0.0025 - learning_rate: 5.9673e-04\n",
      "Epoch 104/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - loss: 0.0025 - val_loss: 0.0018 - learning_rate: 5.9375e-04\n",
      "Epoch 105/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - loss: 0.0020 - val_loss: 0.0026 - learning_rate: 5.9078e-04\n",
      "Epoch 106/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.0019 - val_loss: 0.0024 - learning_rate: 5.8782e-04\n",
      "Epoch 107/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.0022 - val_loss: 0.0010 - learning_rate: 5.8488e-04\n",
      "Epoch 108/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.0019 - val_loss: 0.0020 - learning_rate: 5.8196e-04\n",
      "Epoch 109/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 0.0023 - val_loss: 0.0012 - learning_rate: 5.7905e-04\n",
      "Epoch 110/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0019 - val_loss: 0.0022 - learning_rate: 5.7615e-04\n",
      "Epoch 111/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0022 - val_loss: 0.0018 - learning_rate: 5.7327e-04\n",
      "Epoch 112/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.0020 - val_loss: 0.0014 - learning_rate: 5.7041e-04\n",
      "Epoch 113/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 0.0014 - val_loss: 0.0021 - learning_rate: 5.6756e-04\n",
      "Epoch 114/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.0016 - val_loss: 0.0015 - learning_rate: 5.6472e-04\n",
      "Epoch 115/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 0.0018 - val_loss: 0.0024 - learning_rate: 5.6189e-04\n",
      "Epoch 116/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0017 - val_loss: 0.0028 - learning_rate: 5.5908e-04\n",
      "Epoch 117/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0023 - val_loss: 0.0012 - learning_rate: 5.5629e-04\n",
      "Epoch 118/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0024 - val_loss: 0.0013 - learning_rate: 5.5351e-04\n",
      "Epoch 119/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0018 - val_loss: 0.0013 - learning_rate: 5.5074e-04\n",
      "Epoch 120/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0012 - learning_rate: 5.4799e-04\n",
      "Epoch 121/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0020 - val_loss: 0.0019 - learning_rate: 5.4525e-04\n",
      "Epoch 122/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0015 - val_loss: 0.0020 - learning_rate: 5.4252e-04\n",
      "Epoch 123/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.0021 - learning_rate: 5.3981e-04\n",
      "Epoch 124/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0017 - val_loss: 0.0014 - learning_rate: 5.3711e-04\n",
      "Epoch 125/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0015 - val_loss: 0.0022 - learning_rate: 5.3442e-04\n",
      "Epoch 126/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0021 - val_loss: 0.0013 - learning_rate: 5.3175e-04\n",
      "Epoch 127/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0023 - val_loss: 0.0028 - learning_rate: 5.2909e-04\n",
      "Epoch 128/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0018 - val_loss: 0.0016 - learning_rate: 5.2645e-04\n",
      "Epoch 129/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.0020 - learning_rate: 5.2381e-04\n",
      "Epoch 130/500\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0025 - val_loss: 0.0029 - learning_rate: 5.2120e-04\n",
      "Epoch 130: early stopping\n",
      "Restoring model weights from the end of the best epoch: 100.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "RMSE: 301.8691610046344\n",
      "Training model with n_features=1, dropout_rate=0.2, batch_size=64, patience=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BMVSI-138\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 267ms/step - loss: 0.2856 - val_loss: 0.1179 - learning_rate: 9.9500e-04\n",
      "Epoch 2/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0740 - val_loss: 0.0956 - learning_rate: 9.9003e-04\n",
      "Epoch 3/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0420 - val_loss: 0.0811 - learning_rate: 9.8507e-04\n",
      "Epoch 4/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 0.0336 - val_loss: 0.0675 - learning_rate: 9.8015e-04\n",
      "Epoch 5/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.0359 - val_loss: 0.0573 - learning_rate: 9.7525e-04\n",
      "Epoch 6/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0260 - val_loss: 0.0541 - learning_rate: 9.7037e-04\n",
      "Epoch 7/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0204 - val_loss: 0.0503 - learning_rate: 9.6552e-04\n",
      "Epoch 8/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 0.0200 - val_loss: 0.0497 - learning_rate: 9.6069e-04\n",
      "Epoch 9/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0149 - val_loss: 0.0503 - learning_rate: 9.5589e-04\n",
      "Epoch 10/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0179 - val_loss: 0.0528 - learning_rate: 9.5111e-04\n",
      "Epoch 11/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0141 - val_loss: 0.0541 - learning_rate: 9.4635e-04\n",
      "Epoch 12/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0139 - val_loss: 0.0533 - learning_rate: 9.4162e-04\n",
      "Epoch 13/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0133 - val_loss: 0.0552 - learning_rate: 9.3691e-04\n",
      "Epoch 14/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0104 - val_loss: 0.0573 - learning_rate: 9.3223e-04\n",
      "Epoch 15/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0099 - val_loss: 0.0575 - learning_rate: 9.2757e-04\n",
      "Epoch 16/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0100 - val_loss: 0.0597 - learning_rate: 9.2293e-04\n",
      "Epoch 17/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0095 - val_loss: 0.0592 - learning_rate: 9.1832e-04\n",
      "Epoch 18/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0084 - val_loss: 0.0569 - learning_rate: 9.1372e-04\n",
      "Epoch 19/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0079 - val_loss: 0.0581 - learning_rate: 9.0916e-04\n",
      "Epoch 20/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0085 - val_loss: 0.0568 - learning_rate: 9.0461e-04\n",
      "Epoch 21/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0061 - val_loss: 0.0528 - learning_rate: 9.0009e-04\n",
      "Epoch 22/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 0.0069 - val_loss: 0.0550 - learning_rate: 8.9559e-04\n",
      "Epoch 23/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.0064 - val_loss: 0.0546 - learning_rate: 8.9111e-04\n",
      "Epoch 24/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0074 - val_loss: 0.0527 - learning_rate: 8.8665e-04\n",
      "Epoch 25/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0061 - val_loss: 0.0479 - learning_rate: 8.8222e-04\n",
      "Epoch 26/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0053 - val_loss: 0.0449 - learning_rate: 8.7781e-04\n",
      "Epoch 27/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0058 - val_loss: 0.0422 - learning_rate: 8.7342e-04\n",
      "Epoch 28/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0059 - val_loss: 0.0360 - learning_rate: 8.6905e-04\n",
      "Epoch 29/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.0057 - val_loss: 0.0327 - learning_rate: 8.6471e-04\n",
      "Epoch 30/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0058 - val_loss: 0.0291 - learning_rate: 8.6038e-04\n",
      "Epoch 31/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0045 - val_loss: 0.0245 - learning_rate: 8.5608e-04\n",
      "Epoch 32/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0045 - val_loss: 0.0231 - learning_rate: 8.5180e-04\n",
      "Epoch 33/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0042 - val_loss: 0.0223 - learning_rate: 8.4754e-04\n",
      "Epoch 34/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0046 - val_loss: 0.0150 - learning_rate: 8.4331e-04\n",
      "Epoch 35/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0046 - val_loss: 0.0153 - learning_rate: 8.3909e-04\n",
      "Epoch 36/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0039 - val_loss: 0.0182 - learning_rate: 8.3489e-04\n",
      "Epoch 37/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.0043 - val_loss: 0.0173 - learning_rate: 8.3072e-04\n",
      "Epoch 38/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.0039 - val_loss: 0.0111 - learning_rate: 8.2657e-04\n",
      "Epoch 39/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.0040 - val_loss: 0.0134 - learning_rate: 8.2243e-04\n",
      "Epoch 40/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0035 - val_loss: 0.0105 - learning_rate: 8.1832e-04\n",
      "Epoch 41/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0050 - val_loss: 0.0088 - learning_rate: 8.1423e-04\n",
      "Epoch 42/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0051 - val_loss: 0.0123 - learning_rate: 8.1016e-04\n",
      "Epoch 43/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0031 - val_loss: 0.0178 - learning_rate: 8.0611e-04\n",
      "Epoch 44/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0036 - val_loss: 0.0077 - learning_rate: 8.0208e-04\n",
      "Epoch 45/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0034 - val_loss: 0.0082 - learning_rate: 7.9807e-04\n",
      "Epoch 46/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0029 - val_loss: 0.0059 - learning_rate: 7.9408e-04\n",
      "Epoch 47/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.0031 - val_loss: 0.0057 - learning_rate: 7.9011e-04\n",
      "Epoch 48/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 0.0033 - val_loss: 0.0078 - learning_rate: 7.8615e-04\n",
      "Epoch 49/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0030 - val_loss: 0.0114 - learning_rate: 7.8222e-04\n",
      "Epoch 50/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0029 - val_loss: 0.0098 - learning_rate: 7.7831e-04\n",
      "Epoch 51/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0029 - val_loss: 0.0125 - learning_rate: 7.7442e-04\n",
      "Epoch 52/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0026 - val_loss: 0.0066 - learning_rate: 7.7055e-04\n",
      "Epoch 53/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0036 - val_loss: 0.0160 - learning_rate: 7.6670e-04\n",
      "Epoch 54/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0032 - val_loss: 0.0115 - learning_rate: 7.6286e-04\n",
      "Epoch 55/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0030 - val_loss: 0.0047 - learning_rate: 7.5905e-04\n",
      "Epoch 56/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0038 - val_loss: 0.0023 - learning_rate: 7.5525e-04\n",
      "Epoch 57/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0025 - val_loss: 0.0041 - learning_rate: 7.5148e-04\n",
      "Epoch 58/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - val_loss: 0.0201 - learning_rate: 7.4772e-04\n",
      "Epoch 59/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031 - val_loss: 0.0126 - learning_rate: 7.4398e-04\n",
      "Epoch 60/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0037 - val_loss: 0.0095 - learning_rate: 7.4026e-04\n",
      "Epoch 61/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0026 - val_loss: 0.0024 - learning_rate: 7.3656e-04\n",
      "Epoch 62/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0027 - val_loss: 0.0050 - learning_rate: 7.3288e-04\n",
      "Epoch 63/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.0028 - val_loss: 0.0039 - learning_rate: 7.2921e-04\n",
      "Epoch 64/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0022 - val_loss: 0.0030 - learning_rate: 7.2557e-04\n",
      "Epoch 65/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 0.0029 - val_loss: 0.0048 - learning_rate: 7.2194e-04\n",
      "Epoch 66/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0026 - val_loss: 0.0061 - learning_rate: 7.1833e-04\n",
      "Epoch 67/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0023 - val_loss: 0.0122 - learning_rate: 7.1474e-04\n",
      "Epoch 68/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0026 - val_loss: 0.0121 - learning_rate: 7.1116e-04\n",
      "Epoch 69/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0035 - val_loss: 0.0037 - learning_rate: 7.0761e-04\n",
      "Epoch 70/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 0.0029 - val_loss: 0.0058 - learning_rate: 7.0407e-04\n",
      "Epoch 71/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 0.0020 - val_loss: 0.0019 - learning_rate: 7.0055e-04\n",
      "Epoch 72/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0029 - val_loss: 0.0088 - learning_rate: 6.9705e-04\n",
      "Epoch 73/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0031 - val_loss: 0.0020 - learning_rate: 6.9356e-04\n",
      "Epoch 74/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0030 - val_loss: 0.0019 - learning_rate: 6.9009e-04\n",
      "Epoch 75/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0034 - val_loss: 0.0056 - learning_rate: 6.8664e-04\n",
      "Epoch 76/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - val_loss: 0.0118 - learning_rate: 6.8321e-04\n",
      "Epoch 77/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0126 - learning_rate: 6.7979e-04\n",
      "Epoch 78/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0024 - val_loss: 0.0023 - learning_rate: 6.7639e-04\n",
      "Epoch 79/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0026 - val_loss: 0.0044 - learning_rate: 6.7301e-04\n",
      "Epoch 80/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0033 - val_loss: 0.0062 - learning_rate: 6.6965e-04\n",
      "Epoch 81/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0054 - learning_rate: 6.6630e-04\n",
      "Epoch 82/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0023 - val_loss: 0.0076 - learning_rate: 6.6297e-04\n",
      "Epoch 83/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0077 - learning_rate: 6.5965e-04\n",
      "Epoch 84/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0026 - val_loss: 0.0084 - learning_rate: 6.5636e-04\n",
      "Epoch 85/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - val_loss: 0.0049 - learning_rate: 6.5307e-04\n",
      "Epoch 86/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0026 - val_loss: 0.0014 - learning_rate: 6.4981e-04\n",
      "Epoch 87/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0019 - val_loss: 0.0021 - learning_rate: 6.4656e-04\n",
      "Epoch 88/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0028 - learning_rate: 6.4333e-04\n",
      "Epoch 89/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.0025 - val_loss: 0.0024 - learning_rate: 6.4011e-04\n",
      "Epoch 90/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0027 - val_loss: 0.0029 - learning_rate: 6.3691e-04\n",
      "Epoch 91/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0026 - val_loss: 0.0069 - learning_rate: 6.3372e-04\n",
      "Epoch 92/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0029 - val_loss: 0.0070 - learning_rate: 6.3056e-04\n",
      "Epoch 93/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0026 - val_loss: 0.0036 - learning_rate: 6.2740e-04\n",
      "Epoch 94/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0023 - val_loss: 0.0030 - learning_rate: 6.2427e-04\n",
      "Epoch 95/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0024 - val_loss: 0.0032 - learning_rate: 6.2114e-04\n",
      "Epoch 96/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0018 - val_loss: 0.0031 - learning_rate: 6.1804e-04\n",
      "Epoch 97/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0058 - learning_rate: 6.1495e-04\n",
      "Epoch 98/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022 - val_loss: 0.0017 - learning_rate: 6.1187e-04\n",
      "Epoch 99/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0018 - val_loss: 0.0011 - learning_rate: 6.0881e-04\n",
      "Epoch 100/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0023 - val_loss: 0.0051 - learning_rate: 6.0577e-04\n",
      "Epoch 101/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0021 - val_loss: 0.0034 - learning_rate: 6.0274e-04\n",
      "Epoch 102/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0026 - val_loss: 0.0061 - learning_rate: 5.9973e-04\n",
      "Epoch 103/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0023 - val_loss: 0.0030 - learning_rate: 5.9673e-04\n",
      "Epoch 104/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0024 - val_loss: 0.0097 - learning_rate: 5.9375e-04\n",
      "Epoch 105/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0024 - learning_rate: 5.9078e-04\n",
      "Epoch 106/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0019 - val_loss: 0.0037 - learning_rate: 5.8782e-04\n",
      "Epoch 107/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0027 - val_loss: 0.0029 - learning_rate: 5.8488e-04\n",
      "Epoch 108/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0082 - learning_rate: 5.8196e-04\n",
      "Epoch 109/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - val_loss: 0.0101 - learning_rate: 5.7905e-04\n",
      "Epoch 110/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0024 - val_loss: 0.0036 - learning_rate: 5.7615e-04\n",
      "Epoch 111/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0017 - val_loss: 0.0020 - learning_rate: 5.7327e-04\n",
      "Epoch 112/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0022 - val_loss: 0.0035 - learning_rate: 5.7041e-04\n",
      "Epoch 113/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0021 - val_loss: 0.0063 - learning_rate: 5.6756e-04\n",
      "Epoch 114/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0018 - val_loss: 0.0024 - learning_rate: 5.6472e-04\n",
      "Epoch 115/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0023 - val_loss: 0.0022 - learning_rate: 5.6189e-04\n",
      "Epoch 116/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0020 - val_loss: 0.0049 - learning_rate: 5.5908e-04\n",
      "Epoch 117/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0018 - val_loss: 0.0012 - learning_rate: 5.5629e-04\n",
      "Epoch 118/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0019 - val_loss: 0.0035 - learning_rate: 5.5351e-04\n",
      "Epoch 119/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 0.0017 - learning_rate: 5.5074e-04\n",
      "Epoch 120/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0022 - val_loss: 0.0014 - learning_rate: 5.4799e-04\n",
      "Epoch 121/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0019 - val_loss: 0.0025 - learning_rate: 5.4525e-04\n",
      "Epoch 122/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016 - val_loss: 0.0039 - learning_rate: 5.4252e-04\n",
      "Epoch 123/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0023 - val_loss: 0.0056 - learning_rate: 5.3981e-04\n",
      "Epoch 124/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0018 - val_loss: 8.9445e-04 - learning_rate: 5.3711e-04\n",
      "Epoch 125/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0023 - val_loss: 0.0014 - learning_rate: 5.3442e-04\n",
      "Epoch 126/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0017 - val_loss: 0.0040 - learning_rate: 5.3175e-04\n",
      "Epoch 127/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0019 - val_loss: 0.0015 - learning_rate: 5.2909e-04\n",
      "Epoch 128/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0020 - val_loss: 0.0019 - learning_rate: 5.2645e-04\n",
      "Epoch 129/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0019 - val_loss: 0.0028 - learning_rate: 5.2381e-04\n",
      "Epoch 130/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0020 - val_loss: 0.0058 - learning_rate: 5.2120e-04\n",
      "Epoch 131/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0024 - val_loss: 0.0038 - learning_rate: 5.1859e-04\n",
      "Epoch 132/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0015 - val_loss: 0.0097 - learning_rate: 5.1600e-04\n",
      "Epoch 133/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0024 - val_loss: 0.0060 - learning_rate: 5.1342e-04\n",
      "Epoch 134/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0020 - val_loss: 0.0019 - learning_rate: 5.1085e-04\n",
      "Epoch 135/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0023 - val_loss: 0.0047 - learning_rate: 5.0830e-04\n",
      "Epoch 136/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0023 - val_loss: 0.0077 - learning_rate: 5.0575e-04\n",
      "Epoch 137/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0060 - learning_rate: 5.0322e-04\n",
      "Epoch 138/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0025 - val_loss: 0.0106 - learning_rate: 5.0071e-04\n",
      "Epoch 139/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0020 - val_loss: 0.0039 - learning_rate: 4.9821e-04\n",
      "Epoch 140/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0017 - val_loss: 0.0026 - learning_rate: 4.9571e-04\n",
      "Epoch 141/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 0.0015 - val_loss: 0.0049 - learning_rate: 4.9324e-04\n",
      "Epoch 142/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0020 - val_loss: 0.0035 - learning_rate: 4.9077e-04\n",
      "Epoch 143/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0020 - val_loss: 0.0056 - learning_rate: 4.8832e-04\n",
      "Epoch 144/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0020 - val_loss: 0.0027 - learning_rate: 4.8587e-04\n",
      "Epoch 145/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0017 - val_loss: 0.0054 - learning_rate: 4.8344e-04\n",
      "Epoch 146/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0020 - val_loss: 0.0072 - learning_rate: 4.8103e-04\n",
      "Epoch 147/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0024 - val_loss: 0.0073 - learning_rate: 4.7862e-04\n",
      "Epoch 148/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0020 - val_loss: 0.0037 - learning_rate: 4.7623e-04\n",
      "Epoch 149/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016 - val_loss: 9.2959e-04 - learning_rate: 4.7385e-04\n",
      "Epoch 150/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - loss: 0.0022 - val_loss: 0.0028 - learning_rate: 4.7148e-04\n",
      "Epoch 151/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0018 - val_loss: 0.0021 - learning_rate: 4.6912e-04\n",
      "Epoch 152/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0020 - val_loss: 0.0022 - learning_rate: 4.6678e-04\n",
      "Epoch 153/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0022 - val_loss: 0.0034 - learning_rate: 4.6444e-04\n",
      "Epoch 154/500\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0025 - val_loss: 0.0013 - learning_rate: 4.6212e-04\n",
      "Epoch 154: early stopping\n",
      "Restoring model weights from the end of the best epoch: 124.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "RMSE: 301.90220252082673\n",
      "Best parameters: {'batch_size': 16, 'patience': 30}\n",
      "Best RMSE: 245.0606801167864\n",
      "        Actual  Predicted\n",
      "0   333.444444   1.098885\n",
      "1   337.555556   1.466640\n",
      "2   341.666667   2.640996\n",
      "3   337.666667   7.504671\n",
      "4   336.000000  31.163055\n",
      "5   330.000000  79.714661\n",
      "6   324.000000  80.888893\n",
      "7   318.000000  80.708893\n",
      "8   312.500000  80.734810\n",
      "9   307.000000  80.730942\n",
      "10  298.333333  80.731522\n",
      "11  289.666667  80.731407\n",
      "12  281.000000  80.731415\n",
      "13  279.000000  80.731415\n",
      "14  277.000000  80.731415\n",
      "15  269.000000  80.731415\n",
      "16  266.750000  80.731415\n",
      "17  264.500000  80.731415\n",
      "18  262.250000  80.731415\n",
      "19  260.000000  80.731415\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_data(data, n_features):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_features):\n",
    "        X.append(data[i:i + n_features])\n",
    "        y.append(data[i + n_features])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    return lr * 0.995\n",
    "\n",
    "# Load and scale data\n",
    "time_series_data = df['reading'].values\n",
    "scaler = MinMaxScaler()\n",
    "time_series_data = scaler.fit_transform(time_series_data.reshape(-1, 1)).flatten()\n",
    "\n",
    "batch_size_lst = [16, 32, 64]\n",
    "patience_lst = [30]\n",
    "\n",
    "best_rmse = float('inf')\n",
    "best_parameters = {}\n",
    "best_predictions = None\n",
    "best_model = None\n",
    "\n",
    "# Train and evaluate model function\n",
    "def train_and_evaluate_model(batch_size, patience):\n",
    "    n_feat = 1  # Using only one feature\n",
    "    train_data_scaled = time_series_data[:-20]\n",
    "    test_data_scaled = time_series_data[-(n_feat + 20):]\n",
    "\n",
    "    X_train, y_train = prepare_data(train_data_scaled, n_feat)\n",
    "    X_test, y_test = prepare_data(test_data_scaled, n_feat)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=200, activation='tanh', input_shape=(n_feat, 1), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Bidirectional(LSTM(units=150, activation='relu', return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Bidirectional(LSTM(units=100, activation='relu', return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(units=50, activation='relu', return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=patience, verbose=1, restore_best_weights=True)\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=500, batch_size=batch_size, verbose=1, validation_split=0.2, callbacks=[early_stop, lr_scheduler])\n",
    "\n",
    "    predictions = []\n",
    "    curr_sequence = X_test[0].reshape(1, n_feat, 1)\n",
    "    for i in range(20):\n",
    "        next_pred = model.predict(curr_sequence)\n",
    "        predictions.append(next_pred[0, 0])\n",
    "        curr_sequence = np.append(curr_sequence[:, 1:, :], next_pred.reshape(1, 1, 1), axis=1)\n",
    "\n",
    "    predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
    "    actual = checked_df['reading'].values[-20:]\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predictions))\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "\n",
    "    return rmse, predictions, model\n",
    "\n",
    "for batch_size in batch_size_lst:\n",
    "    for patience in patience_lst:\n",
    "        print(f\"Training model with n_features=1, dropout_rate=0.2, batch_size={batch_size}, patience={patience}\")\n",
    "        rmse, predictions, model = train_and_evaluate_model(batch_size, patience)\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_parameters = {'batch_size': batch_size, 'patience': patience}\n",
    "            best_predictions = predictions\n",
    "            best_model = model\n",
    "\n",
    "print(f\"Best parameters: {best_parameters}\")\n",
    "print(f\"Best RMSE: {best_rmse}\")\n",
    "\n",
    "# Print the results\n",
    "df_results = pd.DataFrame({'Actual': checked_df['reading'].values[-20:], 'Predicted': best_predictions})\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>333.444444</td>\n",
       "      <td>1.098885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>337.555556</td>\n",
       "      <td>1.466640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>341.666667</td>\n",
       "      <td>2.640996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>337.666667</td>\n",
       "      <td>7.504671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>336.000000</td>\n",
       "      <td>31.163055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>330.000000</td>\n",
       "      <td>79.714661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>324.000000</td>\n",
       "      <td>80.888893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>318.000000</td>\n",
       "      <td>80.708893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>312.500000</td>\n",
       "      <td>80.734810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>307.000000</td>\n",
       "      <td>80.730942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>298.333333</td>\n",
       "      <td>80.731522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>289.666667</td>\n",
       "      <td>80.731407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>281.000000</td>\n",
       "      <td>80.731415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>279.000000</td>\n",
       "      <td>80.731415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>277.000000</td>\n",
       "      <td>80.731415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>269.000000</td>\n",
       "      <td>80.731415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>266.750000</td>\n",
       "      <td>80.731415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>264.500000</td>\n",
       "      <td>80.731415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>262.250000</td>\n",
       "      <td>80.731415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>260.000000</td>\n",
       "      <td>80.731415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Actual  Predicted\n",
       "0   333.444444   1.098885\n",
       "1   337.555556   1.466640\n",
       "2   341.666667   2.640996\n",
       "3   337.666667   7.504671\n",
       "4   336.000000  31.163055\n",
       "5   330.000000  79.714661\n",
       "6   324.000000  80.888893\n",
       "7   318.000000  80.708893\n",
       "8   312.500000  80.734810\n",
       "9   307.000000  80.730942\n",
       "10  298.333333  80.731522\n",
       "11  289.666667  80.731407\n",
       "12  281.000000  80.731415\n",
       "13  279.000000  80.731415\n",
       "14  277.000000  80.731415\n",
       "15  269.000000  80.731415\n",
       "16  266.750000  80.731415\n",
       "17  264.500000  80.731415\n",
       "18  262.250000  80.731415\n",
       "19  260.000000  80.731415"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 0-dimensional, but 1 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m next_pred \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(curr_sequence\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     10\u001b[0m predictions\u001b[38;5;241m.\u001b[39mappend(next_pred[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 11\u001b[0m curr_sequence \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcurr_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m, next_pred)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 0-dimensional, but 1 were indexed"
     ]
    }
   ],
   "source": [
    "one_value = [259]\n",
    "# Now using the above trained model I want the next 20 values\n",
    "\n",
    "# Now using the model.predict() function to get the next 20 values\n",
    "predictions = []\n",
    "\n",
    "curr_sequence = one_value\n",
    "for i in range(20):\n",
    "    next_pred = best_model.predict(curr_sequence.reshape(1, 1, 1))\n",
    "    predictions.append(next_pred[0, 0])\n",
    "    curr_sequence = np.append(curr_sequence[1:], next_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
